{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import statsmodels.api as sm\n",
    "import pickle\n",
    "\n",
    "plt.rcParams['font.family'] = 'DeJavu Serif'\n",
    "plt.rcParams['font.serif'] = ['Times New Roman']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load results file\n",
    "def extract_data_from_obj(obj):\n",
    "    samples = []\n",
    "    labels = []\n",
    "    pz_decay = []\n",
    "    item = []\n",
    "    pz_empowered_mu = []\n",
    "    for data in obj:\n",
    "        samples.append(data['sampled_s'])\n",
    "        labels.append(data['label'])\n",
    "        pz_decay.append(data['pz_decay'])\n",
    "        item.append(data['skill'])\n",
    "        pz_empowered_mu.append(data['pz_empowered_mu'])\n",
    "    labels = torch.cat(labels,0)\n",
    "    pz_decay = torch.cat(pz_decay, 0)\n",
    "    items = torch.cat(item, 0)\n",
    "    sampled_s = torch.cat(samples, 1)\n",
    "    \n",
    "    return labels, pz_decay, items, sampled_s\n",
    "\n",
    "object_file = '../groupkt.obj'\n",
    "\n",
    "# Behavioural data\n",
    "file = open(object_file,'rb')\n",
    "obj = pickle.load(file)\n",
    "file.close()\n",
    "perf, decay, items, s_sample = extract_data_from_obj(obj)\n",
    "items = items.detach().cpu().numpy()\n",
    "perf = perf.detach().cpu().numpy()\n",
    "\n",
    "# Graph data\n",
    "model_path = '../groupkt.pt'\n",
    "groupkt = torch.load(model_path)\n",
    "node_emb = groupkt['node_dist.node_u']\n",
    "node_trans = groupkt['node_dist.transformation_layer']\n",
    "\n",
    "# p of direction\n",
    "w_direction = node_emb @ (node_trans - node_trans.transpose(-1,-2)) @ node_emb.transpose(-1,-2)\n",
    "p_direction = torch.sigmoid(w_direction).cpu().numpy()\n",
    "\n",
    "# p of existing\n",
    "num_node = 263 # Assist12\n",
    "w_similarity = node_emb @ node_emb.transpose(-1,-2)\n",
    "w_similarity = w_similarity.cpu() * (1-torch.eye(num_node))\n",
    "p_sim = torch.sigmoid(w_similarity).numpy()\n",
    "sim_origin = w_similarity.numpy()\n",
    "\n",
    "# p of both\n",
    "p_edge = p_sim * p_direction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract transition matrix\n",
    "num_node = 263 # Assist12\n",
    "gap = 1\n",
    "T = np.zeros((num_node, num_node, 4)) # 0-1, 0-0, 1-1, 1-0\n",
    "N = np.zeros((num_node, num_node))\n",
    "\n",
    "for l in range(items.shape[0]):\n",
    "    correct = perf[l]\n",
    "    index = items[l]\n",
    "    \n",
    "    for i in range(10-gap):\n",
    "        if index[i+gap] != index[i]:\n",
    "            if correct[i] == 0:\n",
    "                if correct[i+gap] == 1:\n",
    "                    T[index[i], index[i+gap], 0]+=1\n",
    "                else:\n",
    "                    T[index[i], index[i+gap], 1]+=1\n",
    "            else:\n",
    "                if correct[i+gap] == 1:\n",
    "                    T[index[i], index[i+gap], 2]+=1\n",
    "                else:\n",
    "                    T[index[i], index[i+gap], 3]+=1\n",
    "            N[index[i], index[i+gap]] += 1\n",
    "            \n",
    "print('maximum transition times: ', N.max())\n",
    "print('amount of different transition: ', (N!=0).sum())\n",
    "Nc_minus = T[..., 0] + T[..., 1]\n",
    "Nc_plus = T[..., 2] + T[..., 3]\n",
    "Ne_minus = T[..., 1] + T[..., 3]\n",
    "Ne_plus = T[..., 0] + T[..., 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_sample = 10000\n",
    "\n",
    "# Compute likelihood based on behavioural data\n",
    "# P(D|G0)\n",
    "w0 = np.arange(0,num_sample,1)/num_sample\n",
    "w0 = w0.reshape(num_sample, 1, 1).repeat(num_node, 1).repeat(num_node, -1)\n",
    "p0 = np.power(w0, np.expand_dims(Ne_plus, 0).repeat(num_sample, 0)) * np.power(1-w0, np.expand_dims(Ne_minus, 0).repeat(num_sample, 0))\n",
    "\n",
    "# P (D|G1)\n",
    "w0 = np.arange(0,num_sample,1)/num_sample\n",
    "w0 = w0.reshape(num_sample, 1, 1).repeat(num_node, 1).repeat(num_node, -1)\n",
    "w0 = w0.repeat(num_sample, 0)\n",
    "w1 = np.arange(0,num_sample,1)/num_sample \n",
    "w1 = w1.reshape(num_sample, 1, 1).repeat(num_node, 1).repeat(num_node, -1)\n",
    "w1 = np.tile(w1, (num_sample, 1, 1))\n",
    "\n",
    "N_e1_c1 = np.expand_dims(T[..., 2], 0).repeat(num_sample*num_sample, 0)\n",
    "p_e1_c1 = np.power(np.multiply(w0, 1-w1), N_e1_c1)\n",
    "\n",
    "N_e1_c0 = np.expand_dims(T[..., 0], 0).repeat(num_sample*num_sample, 0)\n",
    "p_e1_c0 = np.power(w0, N_e1_c0)\n",
    "\n",
    "p1 = np.multiply(p_e1_c1, p_e1_c0)\n",
    "\n",
    "# causal support\n",
    "support = np.log(p1.mean(0) + 1e-6) - np.log(p0.mean(0) + 1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regression\n",
    "mask = (T[...,2] + T[..., 3] + T[...,0] + T[..., 1] >1)\n",
    "\n",
    "data = {'performance': support[mask].flatten(),\n",
    "        'mu': p_edge[mask].flatten()\n",
    "       }\n",
    "data = pd.DataFrame(data)\n",
    "data = sm.add_constant(data)\n",
    "\n",
    "y = data['performance']\n",
    "X = data[['const', 'mu']]\n",
    "\n",
    "model = sm.OLS(y, X)\n",
    "result = model.fit()\n",
    "\n",
    "print(result.summary())"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
